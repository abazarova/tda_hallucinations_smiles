{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "from scripts import ripser_count, stats_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mx_filename = lambda i: f\"assets/attention_maps/qa/pt_{i}/attn_matrices.npz\"\n",
    "ntokens_filename = lambda i: f\"assets/attention_maps/qa/pt_{i}/tokens_count.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(\"assets/tda_features\")\n",
    "save_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical features calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_name = \"s_e_v_c_b0b1\"\n",
    "stats_cap = 500\n",
    "\n",
    "thresholds = [0.025, 0.05, 0.1, 0.25, 0.5, 0.75] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(adj_matrices: list, ntokens_array: list, num_of_workers: int = 20):\n",
    "    split_adj_matricies = np.array_split(adj_matrices, num_of_workers)\n",
    "    split_ntokens = np.array_split(ntokens_array, num_of_workers)\n",
    "    assert all([len(m)==len(n) for m, n in zip(split_adj_matricies, split_ntokens)]), \"Split is not valid!\"\n",
    "    return zip(split_adj_matricies, split_ntokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_workers = 20\n",
    "pool = Pool(num_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [11:33<00:00, 693.28s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [09:34<00:00, 17.94s/it]\n",
      "100%|██████████| 32/32 [10:00<00:00, 18.77s/it]\n",
      "100%|██████████| 32/32 [10:01<00:00, 18.81s/it]\n",
      "100%|██████████| 32/32 [10:29<00:00, 19.67s/it]\n",
      "100%|██████████| 32/32 [10:29<00:00, 19.69s/it]\n",
      "100%|██████████| 32/32 [10:38<00:00, 19.96s/it]\n",
      "100%|██████████| 32/32 [10:45<00:00, 20.16s/it]\n",
      "100%|██████████| 32/32 [10:51<00:00, 20.37s/it]\n",
      "100%|██████████| 32/32 [10:54<00:00, 20.47s/it]\n",
      "100%|██████████| 32/32 [10:54<00:00, 20.45s/it]\n",
      "100%|██████████| 32/32 [10:55<00:00, 20.49s/it]\n",
      "100%|██████████| 32/32 [10:50<00:00, 20.31s/it]\n",
      "100%|██████████| 32/32 [10:57<00:00, 20.55s/it]\n",
      "100%|██████████| 32/32 [10:56<00:00, 20.52s/it]\n",
      "100%|██████████| 32/32 [10:54<00:00, 20.45s/it]\n",
      "100%|██████████| 32/32 [10:56<00:00, 20.52s/it]\n",
      "100%|██████████| 32/32 [10:56<00:00, 20.52s/it]\n",
      "100%|██████████| 32/32 [10:59<00:00, 20.60s/it]\n",
      "100%|██████████| 32/32 [10:59<00:00, 20.60s/it]\n",
      "100%|██████████| 32/32 [11:00<00:00, 20.64s/it]\n"
     ]
    }
   ],
   "source": [
    "stats_features, keys = [], []\n",
    "for i in trange(1):\n",
    "    attn_matrices = np.load(attn_mx_filename(i))\n",
    "\n",
    "    with open(ntokens_filename(i), \"r\") as f:\n",
    "        ntokens = json.load(f)\n",
    "    \n",
    "    mx_list, ntokens_list = [], []\n",
    "    for key in attn_matrices.keys():\n",
    "        mx_list.append(attn_matrices[key])\n",
    "        ntokens_list.append(ntokens[key])\n",
    "        keys.append(key)\n",
    "\n",
    "    split = split_data(np.asarray(mx_list), np.asarray(ntokens_list), num_of_workers=num_of_workers)\n",
    "    args = [(mxs, thresholds, tokens, stats_name.split(\"_\"), stats_cap) for mxs, tokens in split]\n",
    "    stats_features_ = pool.starmap(\n",
    "        stats_count.count_top_stats, args\n",
    "    )\n",
    "    stats_features.append(np.concatenate([_ for _ in stats_features_], axis=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stats_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stats_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(\u001b[43mstats_features\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      2\u001b[0m stats_features_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(keys, stats_features\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stats_features' is not defined"
     ]
    }
   ],
   "source": [
    "stats_features = np.concatenate(stats_features, axis=3)\n",
    "stats_features_dict = dict(zip(keys, stats_features.transpose(3, 0, 1, 2, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(f\"{save_path}/stats_features\", **stats_features_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barcodes calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 1\n",
    "lower_bound = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "\n",
    "def subprocess_wrap(queue, function, args):\n",
    "    queue.put(function(*args))\n",
    "    queue.close()\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_only_barcodes(adj_matricies, ntokens_array, dim, lower_bound):\n",
    "    \"\"\"Get barcodes from adj matricies for each layer, head\"\"\"\n",
    "    barcodes = {}\n",
    "    layers, heads = range(adj_matricies.shape[1]), range(adj_matricies.shape[2])\n",
    "    for (layer, head) in product(layers, heads):\n",
    "        matricies = adj_matricies[:, layer, head, :, :]\n",
    "        barcodes[(layer, head)] = ripser_count.get_barcodes(matricies, ntokens_array, dim, lower_bound, (layer, head))\n",
    "    return barcodes\n",
    "\n",
    "def format_barcodes(barcodes):\n",
    "    \"\"\"Reformat barcodes to json-compatible format\"\"\"\n",
    "    return [{d: b[d].tolist() for d in b} for b in barcodes]\n",
    "\n",
    "def save_barcodes(barcodes, filename):\n",
    "    \"\"\"Save barcodes to file\"\"\"\n",
    "    formatted_barcodes = defaultdict(dict)\n",
    "    for layer, head in barcodes:\n",
    "        formatted_barcodes[layer][head] = format_barcodes(barcodes[(layer, head)])\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(formatted_barcodes, f)\n",
    "    \n",
    "def unite_barcodes(barcodes, barcodes_part):\n",
    "    \"\"\"Unite 2 barcodes\"\"\"\n",
    "    for (layer, head) in barcodes_part:\n",
    "        barcodes[(layer, head)].extend(barcodes_part[(layer, head)])\n",
    "    return barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Process Process-21:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/llm-factuality/miniconda/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/llm-factuality/miniconda/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_102683/2182458175.py\", line 4, in subprocess_wrap\n",
      "    queue.put(function(*args))\n",
      "  File \"/tmp/ipykernel_102683/2137997521.py\", line 10, in get_only_barcodes\n",
      "    barcodes[(layer, head)] = ripser_count.get_barcodes(matricies, ntokens_array, dim, lower_bound, (layer, head))\n",
      "  File \"/app/scripts/ripser_count.py\", line 145, in get_barcodes\n",
      "    matrix = matrix_to_ripser(matrix, ntokens_array[i], lower_bound)\n",
      "  File \"/app/scripts/ripser_count.py\", line 127, in matrix_to_ripser\n",
      "    matrix = (matrix > lower_bound).astype(np.int) * matrix\n",
      "  File \"/home/llm-factuality/miniconda/lib/python3.9/site-packages/numpy/__init__.py\", line 394, in __getattr__\n",
      "    raise AttributeError(__former_attrs__[attr])\n",
      "AttributeError: module 'numpy' has no attribute 'int'.\n",
      "`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  0%|          | 0/1 [21:30<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 29\u001b[0m\n\u001b[1;32m     20\u001b[0m p \u001b[38;5;241m=\u001b[39m Process(\n\u001b[1;32m     21\u001b[0m     target\u001b[38;5;241m=\u001b[39msubprocess_wrap,\n\u001b[1;32m     22\u001b[0m     args\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m p\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 29\u001b[0m barcodes_part \u001b[38;5;241m=\u001b[39m \u001b[43mqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m p\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m     31\u001b[0m p\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/multiprocessing/queues.py:103\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block \u001b[38;5;129;01mand\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock:\n\u001b[0;32m--> 103\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sem\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/multiprocessing/connection.py:216\u001b[0m, in \u001b[0;36m_ConnectionBase.recv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m maxlength \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative maxlength\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 216\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bad_message_length()\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "queue = Queue()\n",
    "number_of_splits = 2\n",
    "keys = []\n",
    "for i in trange(1):\n",
    "    attn_matrices = np.load(attn_mx_filename(i))\n",
    "\n",
    "    with open(ntokens_filename(i), \"r\") as f:\n",
    "        ntokens = json.load(f)\n",
    "\n",
    "    mx_list, ntokens_list = [], []\n",
    "    for key in attn_matrices.keys():\n",
    "        mx_list.append(attn_matrices[key])\n",
    "        ntokens_list.append(ntokens[key])\n",
    "        keys.append(key)\n",
    "    \n",
    "    barcodes = defaultdict(list)\n",
    "\n",
    "    split = split_data(mx_list, ntokens_list, number_of_splits)\n",
    "    for matrices, ntokens in tqdm(split, leave=False):\n",
    "        p = Process(\n",
    "            target=subprocess_wrap,\n",
    "            args=(\n",
    "                queue,\n",
    "                get_only_barcodes,\n",
    "                (matrices, ntokens, dim, lower_bound)\n",
    "            )\n",
    "        )\n",
    "        p.start()\n",
    "        barcodes_part = queue.get()\n",
    "        p.join()\n",
    "        p.close()\n",
    "        \n",
    "        barcodes = unite_barcodes(barcodes, barcodes_part)\n",
    "        \n",
    "    save_barcodes(barcodes, save_path / f\"barcodes_{i}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
