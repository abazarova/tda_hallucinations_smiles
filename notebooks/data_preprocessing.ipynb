{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw RAGTruth preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = []\n",
    "with open(\"assets/data/rag_truth/source_info.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        source.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_source = {elem.pop(\"source_id\") : elem for elem in deepcopy(source)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = []\n",
    "with open(\"assets/data/rag_truth/response.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        response.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = set([elem[\"model\"] for elem in response])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt-3.5-turbo-0613',\n",
       " 'gpt-4-0613',\n",
       " 'llama-2-13b-chat',\n",
       " 'llama-2-70b-chat',\n",
       " 'llama-2-7b-chat',\n",
       " 'mistral-7B-instruct'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_qa = [elem for elem in source if elem[\"task_type\"] == \"QA\"]\n",
    "qa_ids = [elem[\"source_id\"] for elem in sources_qa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llm-factuality/miniconda/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7B-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/llm-factuality/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "HF_TOKEN = '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'\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_qa = []\n",
    "model_ids = [\"llama-2-7b-chat\", \"llama-2-13b-chat\", \"mistral-7B-instruct\"]\n",
    "for elem in response:\n",
    "    if (not elem[\"source_id\"] in qa_ids) or (not elem[\"model\"] in model_ids):\n",
    "        continue\n",
    "    preprocessed_qa.append(elem | {\"prompt\": id_to_source[elem[\"source_id\"]][\"prompt\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(preprocessed_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def text_preprocessing(text: str) -> str:\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r\"(@.*?)[\\s]\", \" \", text)\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r\"&amp;\", \"&\", text)\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = []\n",
    "for _, row in df.iterrows():\n",
    "    text = f\"{row['prompt']} {row['response']}\"\n",
    "    tokens = tokenizer(text_preprocessing(text), \n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True)[\"input_ids\"][0]\n",
    "    n_tokens.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1008.0399999999995)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(n_tokens, q=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.apply(lambda x: len(tokenizer(text_preprocessing(f\"{x['prompt']} {x['response']}\"), return_tensors=\"pt\", add_special_tokens=True)[\"input_ids\"][0]) <= 1024, axis = 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving token sequences for each instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"assets/data/rag_truth/qa_samples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>model</th>\n",
       "      <th>temperature</th>\n",
       "      <th>labels</th>\n",
       "      <th>split</th>\n",
       "      <th>quality</th>\n",
       "      <th>response</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11858</td>\n",
       "      <td>14292</td>\n",
       "      <td>mistral-7B-instruct</td>\n",
       "      <td>0.700</td>\n",
       "      <td>[{'start': 102, 'end': 214, 'text': 'However, ...</td>\n",
       "      <td>train</td>\n",
       "      <td>good</td>\n",
       "      <td>Based on the given passages, there are several...</td>\n",
       "      <td>Briefly answer the following question:\\nbutche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11859</td>\n",
       "      <td>14292</td>\n",
       "      <td>llama-2-7b-chat</td>\n",
       "      <td>0.850</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>good</td>\n",
       "      <td>Based on the provided passages, the butcher sh...</td>\n",
       "      <td>Briefly answer the following question:\\nbutche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>11860</td>\n",
       "      <td>14292</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>0.700</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>good</td>\n",
       "      <td>Based on the given passages, the butcher shop ...</td>\n",
       "      <td>Briefly answer the following question:\\nbutche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11864</td>\n",
       "      <td>14293</td>\n",
       "      <td>mistral-7B-instruct</td>\n",
       "      <td>0.850</td>\n",
       "      <td>[{'start': 0, 'end': 99, 'text': 'Conserving w...</td>\n",
       "      <td>train</td>\n",
       "      <td>good</td>\n",
       "      <td>Conserving water is essential for maintaining ...</td>\n",
       "      <td>Briefly answer the following question:\\ntips h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11865</td>\n",
       "      <td>14293</td>\n",
       "      <td>llama-2-7b-chat</td>\n",
       "      <td>0.925</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>good</td>\n",
       "      <td>Here are some tips for conserving water based ...</td>\n",
       "      <td>Briefly answer the following question:\\ntips h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id  source_id                model  temperature  \\\n",
       "0           0  11858      14292  mistral-7B-instruct        0.700   \n",
       "1           1  11859      14292      llama-2-7b-chat        0.850   \n",
       "2           2  11860      14292     llama-2-13b-chat        0.700   \n",
       "3           3  11864      14293  mistral-7B-instruct        0.850   \n",
       "4           4  11865      14293      llama-2-7b-chat        0.925   \n",
       "\n",
       "                                              labels  split quality  \\\n",
       "0  [{'start': 102, 'end': 214, 'text': 'However, ...  train    good   \n",
       "1                                                 []  train    good   \n",
       "2                                                 []  train    good   \n",
       "3  [{'start': 0, 'end': 99, 'text': 'Conserving w...  train    good   \n",
       "4                                                 []  train    good   \n",
       "\n",
       "                                            response  \\\n",
       "0  Based on the given passages, there are several...   \n",
       "1  Based on the provided passages, the butcher sh...   \n",
       "2  Based on the given passages, the butcher shop ...   \n",
       "3  Conserving water is essential for maintaining ...   \n",
       "4  Here are some tips for conserving water based ...   \n",
       "\n",
       "                                              prompt  \n",
       "0  Briefly answer the following question:\\nbutche...  \n",
       "1  Briefly answer the following question:\\nbutche...  \n",
       "2  Briefly answer the following question:\\nbutche...  \n",
       "3  Briefly answer the following question:\\ntips h...  \n",
       "4  Briefly answer the following question:\\ntips h...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 1024\n",
    "model_name = \"llama-2-7b-chat\"\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModel.from_pretrained(model_id).half()\n",
    "# model.to(\"cuda\")\n",
    "\n",
    "df = df[df[\"model\"] == model_name]\n",
    "ids = list(df[\"id\"].apply(str))\n",
    "sentences = list(df[\"prompt\"] + df[\"response\"])\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer(\n",
    "    [text_preprocessing(s) for s in sentences],\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,  # max length to truncate/pad\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = dict(zip(ids, inputs[\"input_ids\"].cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"assets/attention_maps/{model_name}/input_ids.json\", \"w\") as f:\n",
    "    json.dump(input_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
